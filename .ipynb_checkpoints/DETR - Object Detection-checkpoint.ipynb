{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96235694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "90dfe4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DETR(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, hidden_dim, nheads,\n",
    "                 num_encoder_layers, num_decoder_layers):\n",
    "    \n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone = nn.Sequential(*list(resnet50(pretrained=True).children())[:-2])\n",
    "        \n",
    "        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
    "        \n",
    "        self.transformer = nn.Transformer(hidden_dim, nheads,\n",
    "                                          num_encoder_layers, num_decoder_layers)\n",
    "        \n",
    "        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n",
    "        self.linear_box = nn.Linear(hidden_dim, 4) # Box coords \n",
    "        \n",
    "        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim)) # 100, 256 -> 100 N with 256 dim \n",
    "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2)) # 50, 128\n",
    "        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2)) # 50, 128\n",
    "        \n",
    "        # Pos embedding 100 in number but they are more than 1-dim for each pos so we can add them to the\n",
    "        # outpout of the conv1x1 \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        x = self.backbone(inputs)\n",
    "        # ([1, 2048, 25, 38]) # b, C, h, w \n",
    "        \n",
    "        h = self.conv(x)  # ([1, 256, 25, 38]) # b, C, h, w 1x1 conv project \n",
    "        H, W = h.shape[-2:]\n",
    "        \n",
    "        #print(self.col_embed[:W].shape) -> (38, 128) -> (1, 38, 128) -> (25, 38, 128)\n",
    "        #print(self.row_embed[:H].shape) -> (25, 128) -> (25, 1, 128) -> (25, 38, 128) -> (25, 38, 256) \n",
    "        # -> [950, 256] -> [950, 1, 256] pos \n",
    "        pos = torch.cat([\n",
    "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "        \n",
    "        # h fl -> [1, 256, 950] -> [950, 1, 256] # 950 \"sequence length\"/patches then it uses attention\n",
    "        # Each patch 256 dim info repr  \n",
    "        # so that the obj queries weights them \n",
    "        # q -> [100, 1, 256]\n",
    "        h = self.transformer(pos + h.flatten(2).permute(2, 0, 1),\n",
    "                             self.query_pos.unsqueeze(1))\n",
    "\n",
    "        # h -> # [100, 1, 256]\n",
    "        return self.linear_class(h), self.linear_box(h).sigmoid() # Relatative to imgs -> sigmoid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1f1e2531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41459616\n",
      "torch.Size([950, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "detr = DETR(num_classes=91, hidden_dim=256, nheads=8,\n",
    "                 num_encoder_layers=6, num_decoder_layers=6)\n",
    "\n",
    "print(sum(p.nelement() for p in detr.parameters())) # number of parameters in total\n",
    "\n",
    "detr.eval()\n",
    "inputs = torch.randn(1, 3, 800, 1200) # b, c, h, w \n",
    "logits, bboxes = detr(inputs) # Per box -> perdiction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "89bb0061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 92]) torch.Size([100, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "print(logits.shape, bboxes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11078a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
